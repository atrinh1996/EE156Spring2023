\documentclass [12pt]{article}
\usepackage{epsfig}
\usepackage{enumitem}
\usepackage{amsmath}
% \usepackage[color, leftbars]{changebar}
% \usepackage{fontawesome} 
% \usepackage{caption}
% \usepackage{subcaption}


\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-0.5in}

\setlength{\parindent}{0pt}

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{claim}[theorem]{Claim}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{proof}[theorem]{Proof}

\newlength{\toppush}
\setlength{\toppush}{2\headheight}
\addtolength{\toppush}{\headsep}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue, % was previously black
    filecolor=magenta,
    urlcolor=blue,
    pdftitle={Template}
}
\urlstyle{same}


\def\subjnum{EE 156}
\def\subjname{Adv. Comp. Arch.}

\def\doheading#1#2#3{\vfill\eject\vspace*{-\toppush}%
  \vbox{\hbox to\textwidth{{\bf} \subjnum: \subjname \hfil Amy Bui}%
    \hbox to\textwidth{{\bf} Tufts University, Spring 2023 \hfil#3\strut}%
    \hrule}}

\newcommand{\htitle}[1]{\vspace*{3.25ex plus 1ex minus .2ex}%
\begin{center}
{\large\bf #1}
\end{center}} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\doheading{2}{title}{Paper Review} 
% \htitle{Paper Info}
% \bigskip 
% \bigskip 
%%%%%%%%%% begin text after this line %%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Summary}
    \label{sec:summary}

        \textbf{Designing and Evaluation of Compiler Algorithm for Prefetching (1992):} This is Mowry's 1992 Ph.D. thesis proposing a selective software prefetching algorithm. Their algorithm uses locality analysis, loop splitting, and software pipelining in order to reduce instruction prefetching overhead and stresses on the memory subsystem that were previous issues in software-controlled prefetching. Comparing their results on benchmarks done with no prefetching and indiscriminate prefetching, selective prefetching with loop splitting improved performance. Their conclusions lead to their assertion that, with the cost of complex hardware prefetches, microarchitectures should instead be supporting memory hierarchy optimizations like lockup-free caches and prefetching instructions.  

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \section{Strengths} %%%%%%%%%%%%%%%%%%%%%%
    \label{sec:strengths}
        \begin{itemize}
            \item Figures 3 and 4 were good representations of their performance results of their algorithm against control benchmarks, showing not only how their algorithm improved in overall execution time, but also a breakdown of how instructions and prefetching overhead compared across the benchmarks. 
        \end{itemize}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \section{Weaknesses} %%%%%%%%%%%%%%%%%%%%%%
    \label{sec:weaknesses}
        \begin{itemize}
            \item The setion 2 description of the prefetching algorithm was very verbose, and only referencing the same figure for the entire section made it easier to get lost in the details of the algorithm. 
            \item The scope of their compiler algorithm was limited to affine array accesses within scientific applications, but a more broad application was not included, nor a discussion about feasibility/infeasibility of their algorithm generalized for more kinds of access patterns. 
        \end{itemize}
        % \begin{itemize}
        %     \item Section 2 where they discuss their prefetch algorithm seems more verbose than it needed to be; however, this is coming from the perspective of someone reading their study 30 years later and having learnt about the concept of prefetching and loop unrolling already. The entire section references the same Figure 2 throughout, but it would have been better done both aggregated and broken up, where each new term and strategy they introduced was accompanied by a visual example of how the locality, loop peeling or runrolling, and/or how the prefetching predicate worked with the example. 
        %     \item 633 citations; they do software prefetching which inserts instructions; papers as recently as 2022 still Mowry (1992), but more an more, this seems to be in the context of background information for their papers. This is Mowry's PhD thesis. Other similar papers in the early 90s also covered software controlled prefetching. 
        %     \item The scope of the compiler algorithm was limited to affine array accesses within scientific applications. Could they have also explored other access patterns in this study (mentioned in sec 6)? If not, it makes sense becauase they did things like assumptions about prefetch accesses not being able to be interrupted, for some of the benchmarks, they manually changed the alignments of the matrices to reduce cache conflicts 
        % \end{itemize}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \section{Rating: 3} %%%%%%%%%%%%%%%%%%%%%% 
    \label{sec:rating}
    % \pagebreak
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \section{Comments} %%%%%%%%%%%%%%%%%%%%%%
    \label{sec:comments}

        It is interesting to read early papers on how specific kinds of memory optimizations were approached; in this case, instruction prefetching. Looking at their citations and on ACM, other papers on software-controlled prefetching were also published in the early 90s around the same time as this one, as well. The authors' assertions were correct, in that architectures since the time of their publishing in 1992 do accommodate for more software-controlled optimizations, either given by certain programming language features or new compiler optimizations being released. For a time, though, around the early 2000s, the focus shifted back on the architecture and optimizations on memory organization to improve performance; but, referencing the 2017 Turing Lecture (Hennessey and Patterson), with another shift towards collaborative approaches to designing and optimizing processors (with architects, compiler writers, OS designers, etc.), there may be more opportunities for these optimization software algorithms in the microarchitecture. That being said, given this paper's age and the weaknesses listed above, another more recent paper or study covering prefetching could replace this one in the readings. This paper has had 633 forward citations, but are in the context of background information for these more recent papers, often as a one-sentence reference about the existence of software approaches to memory optimizations. The high level idea of software-controlled algorithms is still relevant but this particular study and their algorithm are less so. 
        
        % One of the weaknesses mentioned above was how they presented their prefetching algorithm in a more verbose way than it needed to be; a caveat being that this is coming from the perspective of someone reading their study 30 years later, being used to different writing conventions, and having learnt about the concept of prefetching and loop unrolling already. All of section 2 references the same Figure 2 throughout, but it would have been better done both aggregated and broken up, where each new term and strategy they introduced was accompanied by a visual example of how the locality, loop peeling or unrolling, and/or how the prefetching predicate worked with the example. 
        
        % Another weakness is the scope of their compiler algorithm was limited to affine array accesses within scientific applications, which they acknowledge in section 6. Whether or not it was possible at the time, I would have liked their discussion to expand on if a more generalized version of their algorithm, even unimplemented, would have worked on other types of access patterns; and if not, then why? 
        
        % Finally, since its publishing, according to ACM, this paper has had 633 forward citations, from papers even as recently as 2022; however, this seems to be in the context of background information for these more recent papers, often a one-sentence reference about the mere existence of a software approach to optimizations. This prefetching paper does not feel as relevant or as exciting to read. 

        

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
        
    % \pagebreak
    % END %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    % \section{Notes}
    %     \begin{itemize}
    %         \item Problems in software-controlled prefetching: incurs instruction overhead, increas load on memory subsystem. 
    %         \item Address: reduce prefetches (overhead) for data already in cache. 
    %         \item Their algorithm: identifies memory references that are likely to be cache misses and only issues prefetch instructions for those.
    %             \begin{itemize}
    %                 \item Greatly improves performance
    %                 \item Better than a prefetch algo that does so indiscriminitely. 
    %             \end{itemize}
    %         \item They assert that future microprocessors should support memory hierarchy optimizations, i.e. lock-up free caches (allow multiple outstanding misses; ie in prefetch, the software uses instructions to pre-fetch/get data before the normal fetch, which is a ``miss'' om a sense) and ISAs should have prefetch instructions. (complex hardware prefetching is not necessary).
    %         \item \textbf{Locality Analysis}:
    %             \begin{itemize}
    %                 \item intrinsit data reuse (in a loop nest): find instances of data (array) accesses that refer to the same cache line. Temporal, spatial, and group reuses. 
    %                 \item exploit set of reuses for cache of particular size
    %             \end{itemize}
    %         \item They demonstrate the benefits of software controlled prefetching, ie speedup in overall performance, with typically small prefetching memory overhead and sometimes the number of instrs actually decreasings due to savings through loop unrolling.
    %         \item I think Fig 3, 4 is good representaation of results. 
    %         \item Aspects of Prefetching alforithm:
    %             \begin{itemize}
    %                 \item Locality analysis - eliminate prefetching overhead by only prefetching for cache miss references (selective prefetching, rather than indiscriminate). Fig. 4 compares performance for no pf, indiscriminate pf, and selective pf. indiscriminate pf suffers from increased instr overhead and stress on memory subsystem, whilc selective pf improve performance and even resulted in gains in some benchmarks. 
    %                 \item Loop splitting: selective pf w/ conditional statements vs. selective pf w/loop splitting. the latter performed better, evidence by the instr overhead per prefetch issue. 
    %                 \item Software Pipelining: 
    %             \end{itemize}
    %     \end{itemize}

% \begin{thebibliography}{1}
%     \bibitem[1]{paper}Todd C. Mowry, Monica S. Lam, and Anoop Gupta. 1992. Design and evaluation of a compiler algorithm for prefetching. In Proceedings of the fifth international conference on Architectural support for programming languages and operating systems (ASPLOS V), Richard L. Wexelblat (Ed.). ACM, New York, NY, USA, 62-73. DOI=\url{http://dx.doi.org/10.1145/143365.143488}
% \end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

